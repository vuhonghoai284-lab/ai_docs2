"""ä»»åŠ¡å¤„ç†æ¨¡å—"""
from datetime import datetime
from typing import List
from sqlalchemy.orm import Session
from database import Task, Issue
from file_parser import FileParser
from ai_service import AIService
from utils.task_logger import TaskLoggerFactory, TaskStage

class TaskProcessor:
    """ä»»åŠ¡å¤„ç†å™¨"""
    
    def __init__(self):
        self.file_parser = FileParser()
        self.ai_service = None  # Will be initialized with db session in process_task
        self.logger = None  # Will be initialized in process_task
    
    def split_text(self, text: str, model_config: dict) -> List[str]:
        """æ–‡æœ¬åˆ†å— - åŸºäºæ¨¡å‹é…ç½®è®¡ç®—åŠ¨æ€chunk_size"""
        # ä»æ¨¡å‹é…ç½®ä¸­è·å–å‚æ•°
        context_window = model_config.get('context_window', 32000)  # é»˜è®¤32k
        reserved_tokens = model_config.get('reserved_tokens', 2000)  # é»˜è®¤é¢„ç•™2000
        
        # è®¡ç®—å¯ç”¨äºæ–‡æ¡£å†…å®¹çš„tokens
        available_tokens = context_window - reserved_tokens
        
        # ç²—ç•¥ä¼°ç®—ï¼š1ä¸ªtokenå¤§çº¦å¯¹åº”1.2-1.5ä¸ªå­—ç¬¦ï¼ˆå¯¹äºä¸­æ–‡å’Œè‹±æ–‡æ··åˆï¼‰
        # ä¸ºäº†ä¿å®ˆèµ·è§ï¼Œä½¿ç”¨1.0çš„æ¯”ä¾‹
        chunk_size = int(available_tokens * 0.8)  # å†ç•™20%çš„ç¼“å†²
        
        if self.logger:
            self.logger.info(f"åŠ¨æ€åˆ†å—é…ç½®: context_window={context_window}, reserved_tokens={reserved_tokens}, "
                            f"available_tokens={available_tokens}, chunk_size={chunk_size}")
        
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        for i in range(0, len(text), chunk_size):
            chunks.append(text[i:i+chunk_size])
        return chunks
    
    async def process_task(self, task_id: int, db: Session):
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        import time
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        
        # è·å–æ—¥å¿—ç®¡ç†å™¨
        logger = await TaskLoggerFactory.get_logger(str(task_id), "task_processor")
        self.logger = logger  # è®¾ç½®å®ä¾‹çº§loggerä¾›split_textä½¿ç”¨
        
        # è·å–ä»»åŠ¡é…ç½®çš„æ¨¡å‹ç´¢å¼•å¹¶åˆå§‹åŒ–AIæœåŠ¡
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            await logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
            return
        
        # è°ƒè¯•ï¼šæ‰“å°ä»æ•°æ®åº“è¯»å–çš„å€¼
        print(f"[DEBUG task_processor] ä»»åŠ¡ {task_id}: æ•°æ®åº“ä¸­çš„model_index={task.model_index}, ç±»å‹={type(task.model_index)}")
        
        # ç¡®ä¿model_indexä¸ä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼0
        model_index = task.model_index if task.model_index is not None else 0
        print(f"[DEBUG task_processor] å¤„ç†åçš„model_index={model_index}, ç±»å‹={type(model_index)}")
        await logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ç´¢å¼•: {model_index} (æ•°æ®åº“å€¼: {task.model_index}, æ¨¡å‹åç§°: {task.model_label})")
        
        # å¦‚æœæ•°æ®åº“ä¸­çš„model_indexä¸ºNoneï¼Œæ›´æ–°ä¸ºé»˜è®¤å€¼
        if task.model_index is None:
            task.model_index = 0
            db.commit()
            await logger.info(f"å·²å°†ä»»åŠ¡ {task_id} çš„model_indexæ›´æ–°ä¸ºé»˜è®¤å€¼0")
        
        try:
            # åˆå§‹åŒ–AIæœåŠ¡ï¼Œä¼ å…¥æ•°æ®åº“ä¼šè¯å’Œæ¨¡å‹ç´¢å¼•
            await logger.info(f"æ­£åœ¨åˆå§‹åŒ–AIæœåŠ¡ï¼Œæ¨¡å‹ç´¢å¼•: {model_index}")
            self.ai_service = AIService(db_session=db, model_index=model_index)
            await logger.info(f"âœ… AIæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            task.status = 'processing'
            task.progress = 0
            db.commit()
            # 1. è§£ææ–‡ä»¶
            await logger.set_stage(TaskStage.PARSING, f"å¼€å§‹è§£ææ–‡ä»¶: {task.file_path}")
            text = self.file_parser.parse(task.file_path)
            
            # ä¿å­˜æ–‡æ¡£å­—ç¬¦æ•°
            task.document_chars = len(text)
            
            # æ›´æ–°è¿›åº¦
            task.progress = 20
            db.commit()
            await logger.progress(20, f"æ–‡ä»¶è§£æå®Œæˆï¼Œæ–‡æ¡£é•¿åº¦: {len(text)} å­—ç¬¦")
            
            # 2. æ–‡æœ¬åˆ†å— - ä½¿ç”¨æ¨¡å‹é…ç½®åŠ¨æ€è®¡ç®—chunk_size
            # è·å–å½“å‰æ¨¡å‹é…ç½®
            model_config = self.ai_service.config
            chunks = self.split_text(text, model_config)
            await logger.info(f"æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…±{len(chunks)}å—", metadata={"chunk_count": len(chunks)})
            
            # 3. AIæ£€æµ‹æ¯ä¸ªæ–‡æœ¬å—ï¼ˆæ”¯æŒè¿›åº¦å›è°ƒï¼‰
            all_issues = []
            
            # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
            async def update_progress(message: str, progress: int):
                """æ›´æ–°ä»»åŠ¡è¿›åº¦å’Œæ¶ˆæ¯"""
                task.progress = min(progress, 95)  # ç•™5%ç»™æœ€åçš„ä¿å­˜æ­¥éª¤
                task.message = message
                db.commit()
                await logger.progress(progress, message)
            
            # è¿›å…¥åˆ†æé˜¶æ®µ
            await logger.set_stage(TaskStage.ANALYZING, "å¼€å§‹AIå†…å®¹åˆ†æ")
            
            for i, chunk in enumerate(chunks):
                await logger.info(f"å¤„ç†ç¬¬{i+1}/{len(chunks)}å—", metadata={"chunk_index": i+1, "total_chunks": len(chunks)})
                
                # è®¡ç®—å½“å‰å—çš„è¿›åº¦èŒƒå›´
                base_progress = 20 + (70 * i / len(chunks))
                chunk_progress_range = 70 / len(chunks)
                
                # åˆ›å»ºå½“å‰å—çš„è¿›åº¦å›è°ƒ
                async def chunk_progress_callback(msg: str, pct: int):
                    """å°†å—å†…çš„è¿›åº¦è½¬æ¢ä¸ºæ€»ä½“è¿›åº¦"""
                    actual_progress = base_progress + (chunk_progress_range * pct / 100)
                    await update_progress(f"[å—{i+1}/{len(chunks)}] {msg}", int(actual_progress))
                
                # ä½¿ç”¨è¿›åº¦å›è°ƒæ£€æµ‹é—®é¢˜ï¼Œä¼ å…¥task_idä»¥ä¿å­˜AIè¾“å‡º
                issues = await self.ai_service.detect_issues(chunk, chunk_progress_callback, task_id)
                
                # æ·»åŠ å—ç´¢å¼•åˆ°ä½ç½®ä¿¡æ¯
                for issue in issues:
                    if len(chunks) > 1:
                        issue['location'] = f"ç¬¬{i+1}éƒ¨åˆ† - {issue.get('location', '')}"
                all_issues.extend(issues)
            
            # 4. ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“
            await logger.set_stage(TaskStage.GENERATING, "æ­£åœ¨ä¿å­˜æ£€æµ‹ç»“æœ...", 95)
            await logger.info(f"æ£€æµ‹åˆ°{len(all_issues)}ä¸ªé—®é¢˜", metadata={"issue_count": len(all_issues)})
            for issue_data in all_issues:
                issue = Issue(
                    task_id=task_id,
                    issue_type=issue_data.get('type', 'æœªçŸ¥'),
                    description=issue_data.get('description', ''),
                    location=issue_data.get('location', ''),
                    severity=issue_data.get('severity', 'ä¸€èˆ¬'),
                    confidence=issue_data.get('confidence', 0.8),  # æ·»åŠ ç½®ä¿¡åº¦å­—æ®µ
                    suggestion=issue_data.get('suggestion', ''),
                    # æ–°å¢å­—æ®µ
                    original_text=issue_data.get('original_text', ''),
                    user_impact=issue_data.get('user_impact', ''),
                    reasoning=issue_data.get('reasoning', ''),
                    context=issue_data.get('context', '')
                )
                db.add(issue)
            
            # 5. å®Œæˆä»»åŠ¡
            task.status = 'completed'
            task.progress = 100
            task.message = f"æ–‡æ¡£æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(all_issues)} ä¸ªé—®é¢˜"
            task.completed_at = datetime.now()
            task.processing_time = time.time() - start_time  # è®¡ç®—æ€»è€—æ—¶
            db.commit()
            
            await logger.set_stage(TaskStage.COMPLETE, f"ä»»åŠ¡{task_id}å¤„ç†å®Œæˆ", 100)
            await TaskLoggerFactory.close_logger(str(task_id))
            
        except Exception as e:
            # å¤„ç†å¤±è´¥
            import traceback
            error_details = traceback.format_exc()
            await logger.error(f"ä»»åŠ¡{task_id}å¤„ç†å¤±è´¥: {str(e)}")
            await logger.error(f"é”™è¯¯å †æ ˆ:\n{error_details}")
            await logger.set_stage(TaskStage.ERROR, f"ä»»åŠ¡å¤„ç†å¤±è´¥: {str(e)}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            task.status = 'failed'
            task.error_message = str(e)
            task.progress = 0
            task.processing_time = time.time() - start_time
            db.commit()
            
            await TaskLoggerFactory.close_logger(str(task_id))

# å…¨å±€ä»»åŠ¡å¤„ç†å™¨å®ä¾‹
task_processor = TaskProcessor()