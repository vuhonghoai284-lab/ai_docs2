项目背景：开发一个AI资料自主测试项目，以用户视角审视待测试资料可能存在的产品问题，整理问题并输出结构化测试报告


 前端功能：
 1、AI资料测试任务模块：
    列表页面：提供快速创建按钮、任务列表，单个任务列表右侧支持删除、查看报告、下载报告、重试、停止等操作
    创建功能：用户仅需要导入需要测试的资料文件（PDF、docs、markdown格式），选择任务类型（静态检测、动态检测），提交即可
    任务执行：用户提交任务后，调用后端API执行任务，实时刷新任务进度和状态
    列表功能：用户可以查询自己所有的任务列表
    任务报告：当任务完成后，用户可以查看和下载报告
2、 查看报告：页面列出AI评估的问题列表，可以对每个问题进行 接受、拒绝、评价等操作，系统会记录用户操作
    下载报告：待用户将问题全部操作完成后，后端会再次结合用户数据生成最终报告，用户可以下载最终报告（excel文件）。


后端功能：
1、通过调用AI大模型接口完成任务执行
2、AI资料测试包含静态检测、动态检测2大部分，需要实现静态检测功能业务实现
3、静态检测：任务执行流程按下如下步骤进行
    1、文件解析：通过不同开源库将pdf、docs、markdown进行文本解析，解析需要将资料文本拆分为指定（通过配置文件配置，默认8000）长度的字符串
    2、文本结构化：将拆分后的文本列表，通过批量方案调用AI大模型按照章节进行结构化提取，统一输出结构化数据（章节名称、正文内容、表格内容、章节摘要、章节类型（操作类、描述类））
    3、静态检测：将需要检测的内容点，包含语法检测、逻辑错误、完整性评估等维度进行分析，将输出结构通过json结构化输出
    4、结构化验证：为了保障数据结构化可靠，将静态检测结果进行本地json解析，解析如果失败，将失败信息传递给大模型，让大模型继续修改调整
4、任务过程中数据库记录大模型每次输入输出、方便后续调试



问题澄清
  1. AI大模型接口:
    - 具体使用哪个AI服务商？（OpenAI、阿里云、百度等）：采用vllm自部署，使用deepseek和qwen模型
    - API调用的具体格式和认证方式？ 测试版本不需要提供认证
    - 是否有调用频率限制？  无
  2. 数据库设计:
    - 使用什么数据库？（MySQL、PostgreSQL等） 测试版本使用sqlite
    - 用户系统如何集成？需要自建用户认证还是对接现有系统？ 当前不需要集成

  业务逻辑细节

  3. 文件处理:
    - 文件上传大小限制？ 10M，可通过配置修改
    - 文件存储方式（本地存储还是云存储）？ 本地存储
    - 解析失败时的处理机制？ API直接报错，页面处理错误
  4. 静态检测具体规则:
    - "语法检测、逻辑错误、完整性评估"的具体评估标准是什么？ 暂时没有，请自行规划
    - 需要预定义检测规则模板吗？ 不需要
  5. 报告生成:
    - Excel报告的具体格式要求？ 请合理设计
    - 用户操作记录（接受、拒绝、评价）的具体数据结构？请合理设计
  6. 任务状态管理:
    - 任务状态有哪些？（等待中、执行中、已完成、失败等）请合理设计
    - 重试机制的具体逻辑？ 用户手动触发

  用户体验 当前不考虑用户权限问题

  7. 权限控制:
    - 是否需要多用户隔离？
    - 是否需要管理员功能？
  8. 实时更新:
    - 任务进度更新采用轮询还是WebSocket？



    │ > 请修改前后端测试模式流程，仅在需要真实调用外部API时进行mock模拟数据返回，业务代码流程和正式环境保持一致，确保测试模式能完全模拟生产环境


    有如下几个bug请修复下，修复完成请添加相关测试用例，并执行全量用例，确保用例全部通过：1、DocumentProcess object has no attribute 
  'analyze_document';2、前端代码存在localhost:8080 
  API调用硬编码，请统一修改从配置或环境变量获取；3、将第三方登录相关url、key等配置加入到config.yaml中进行加载